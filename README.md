# pytorch_train_leaf_morpho
UNET model for variable size inputs generated by leaf morpho tools.

# Directory description
## data
Contains train and validate image and mask sets.
## envs
Contains the package dependencies to run the pipeline.
## notebooks
Contains notebooks with feeback for debugging the pipeline.
notebooks/pytorch_train_UNETV2.ipynb is the current version
notebooks/pytorch_train_UNETV2_no_reduction.ipynb is the next step, and has loss computation to only include the original, unpadded region of interest.
## scripts
.py deployable scripts of the notebooks.
## debugging info:
I've hit a roadblock and could use some help. I've been developing a UNET model in pytorch for semantic segmentation. The initial version of this model successfully runs and trains a model on GPU:
scripts/pytorch_train_fixed.py
However, it has to reduce all image sizes to fixed dimensions that are divisible by 8, e.g. 256x256, 1024x1024. I need the model to use reflective padding to take images of variable sizes and return masks of the same size when used for inference, which requires this model to be trained on padded images using a custom collate function. I've built a notebook to help debug and test-run the pipeline:
notebooks/pytorch_train_UNETV2.ipynb
And a python script for eventual full deployment:
scripts/pytorch_train_UNET_variable_collate.py
The notebook contains feedback that indicates currently the input images/corresponding masks all match in dimensions and the padding is working successfully. However, the dynamic padding results in the output being a slightly different size, causing memory fragmentation (I could be wrong but this error has been difficult to debug and create feedback for).
I'm shelving this project for a little bit, and I'll revisit it on occasion to take stabs at trying to solve it when I have a fresh mind, but the obstacle is turning into a time sink for me and I need to work on other projects for the short term.
If anyone picks it up, feel free to reformat for tensorflow if need be. I can quickly generate masks to train off of currently, so I would prefer to train off of image/mask data format. I do have deployable python solutions for the time being, so this is more of a long-term, crop-agnostic strategy. Anyone who can solve this gets a high five and bragging rights
## resources
This blog posts outlines the approach I need to take:
https://discuss.pytorch.org/t/how-would-you-train-a-segmentation-model-with-varying-image-sizes/183280/2
