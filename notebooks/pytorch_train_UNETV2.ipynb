{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define segmentation dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.image_files = sorted([f for f in os.listdir(image_dir) if not f.startswith('.')])\n",
    "        self.mask_files = sorted([f for f in os.listdir(mask_dir) if not f.startswith('.')])\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.mask_files[idx])\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        image = self.transform(image)\n",
    "        mask = self.transform(mask)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the UNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        def double_conv(in_ch, out_ch):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        self.dconv_down1 = double_conv(in_channels, 64)\n",
    "        self.dconv_down2 = double_conv(64, 128)\n",
    "        self.dconv_down3 = double_conv(128, 256)\n",
    "        self.dconv_down4 = double_conv(256, 512)\n",
    "\n",
    "        # Modified maxpool with ceil_mode\n",
    "        self.maxpool = nn.MaxPool2d(2, ceil_mode=True)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.dconv_up3 = double_conv(256 + 512, 256)\n",
    "        self.dconv_up2 = double_conv(128 + 256, 128)\n",
    "        self.dconv_up1 = double_conv(64 + 128, 64)\n",
    "\n",
    "        self.conv_last = nn.Conv2d(64, out_channels, 1)\n",
    "\n",
    "    def crop_tensor(self, target_tensor, tensor_to_crop):\n",
    "        _, _, H, W = tensor_to_crop.size()\n",
    "        return target_tensor[:, :, :H, :W]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder path\n",
    "        conv1 = self.dconv_down1(x)\n",
    "        x = self.maxpool(conv1)\n",
    "\n",
    "        conv2 = self.dconv_down2(x)\n",
    "        x = self.maxpool(conv2)\n",
    "\n",
    "        conv3 = self.dconv_down3(x)\n",
    "        x = self.maxpool(conv3)\n",
    "\n",
    "        # Bottleneck\n",
    "        x = self.dconv_down4(x)\n",
    "\n",
    "        # Decoder path with cropping\n",
    "        x = self.upsample(x)\n",
    "        conv3 = self.crop_tensor(conv3, x)\n",
    "        x = torch.cat([x, conv3], dim=1)\n",
    "        x = self.dconv_up3(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        conv2 = self.crop_tensor(conv2, x)\n",
    "        x = torch.cat([x, conv2], dim=1)\n",
    "        x = self.dconv_up2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        conv1 = self.crop_tensor(conv1, x)\n",
    "        x = torch.cat([x, conv1], dim=1)\n",
    "        x = self.dconv_up1(x)\n",
    "\n",
    "        out = self.conv_last(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a collate function with padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images, masks = zip(*batch)\n",
    "\n",
    "    max_height = max(img.shape[-2] for img in images)\n",
    "    max_width = max(img.shape[-1] for img in images)\n",
    "\n",
    "    # Ensure the dimensions are divisible by 8\n",
    "    target_height = ((max_height + 7) // 8) * 8\n",
    "    target_width = ((max_width + 7) // 8) * 8\n",
    "\n",
    "    def pad_to_size(tensor, target_height, target_width, mode='reflect'):\n",
    "        if tensor.ndim == 3:\n",
    "            tensor = tensor.unsqueeze(0)  # Add batch dimension\n",
    "        _, _, h, w = tensor.shape\n",
    "        pad_h = target_height - h\n",
    "        pad_w = target_width - w\n",
    "        padded_tensor = F.pad(tensor, (0, pad_w, 0, pad_h), mode=mode)\n",
    "        return padded_tensor.squeeze(0)  # Remove batch dimension\n",
    "\n",
    "    padded_images = [pad_to_size(img, target_height, target_width, mode='reflect') for img in images]\n",
    "    padded_masks = [pad_to_size(msk.unsqueeze(0), target_height, target_width, mode='constant') for msk in masks]\n",
    "\n",
    "    # Check if all padded images have the same shape\n",
    "    assert all(img.shape == padded_images[0].shape for img in padded_images), \"Padded images have inconsistent shapes\"\n",
    "\n",
    "    # Check if all padded masks have the same shape \n",
    "    assert all(msk.shape == padded_masks[0].shape for msk in padded_masks), \"Padded masks have inconsistent shapes\"\n",
    "\n",
    "    # Print shapes for debugging\n",
    "    print(\"Padded images shapes:\", [img.shape for img in padded_images])\n",
    "    print(\"Padded masks shapes:\", [msk.shape for msk in padded_masks])\n",
    "\n",
    "    return torch.stack(padded_images), torch.stack(padded_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0 reduction update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## upload train/val sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize datasets\n",
    "train_dataset = SegmentationDataset(\n",
    "    image_dir=\"/Users/aja294/Documents/Hemp_local/leaf_morphometrics/semantic_seg_template/data/train/images\",\n",
    "    mask_dir=\"/Users/aja294/Documents/Hemp_local/leaf_morphometrics/semantic_seg_template/data/train/masks\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = SegmentationDataset(\n",
    "    image_dir=\"/Users/aja294/Documents/Hemp_local/leaf_morphometrics/semantic_seg_template/data/val/images\",\n",
    "    mask_dir=\"/Users/aja294/Documents/Hemp_local/leaf_morphometrics/semantic_seg_template/data/val/masks\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Metal/Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "model = UNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "model = UNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Updated model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 1\n",
      "Padded images shapes: [torch.Size([3, 2528, 2728]), torch.Size([3, 2528, 2728]), torch.Size([3, 2528, 2728]), torch.Size([3, 2528, 2728])]\n",
      "Padded masks shapes: [torch.Size([1, 2528, 2728]), torch.Size([1, 2528, 2728]), torch.Size([1, 2528, 2728]), torch.Size([1, 2528, 2728])]\n",
      "Training batch shapes - Images: torch.Size([4, 3, 2528, 2728]), Masks: torch.Size([4, 1, 2528, 2728])\n",
      "Training batch shapes (on device) - Images: torch.Size([4, 3, 2528, 2728]), Masks: torch.Size([4, 1, 2528, 2728])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Starting training epoch {epoch+1}\")  # Debug: Ensure entering the training loop\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for images, masks in train_loader:\n",
    "        print(f\"Training batch shapes - Images: {images.shape}, Masks: {masks.shape}\")  # Debug: Check input shapes\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        print(f\"Training batch shapes (on device) - Images: {images.shape}, Masks: {masks.shape}\")  # Debug: Check shapes after moving to device\n",
    "\n",
    "        outputs = model(images)\n",
    "        print(\"Model output shape:\", outputs.shape)  # Debug: Check model output shape\n",
    "        print(\"Target masks shape:\", masks.shape)  # Debug: Check target masks shape\n",
    "\n",
    "        # Temporarily remove loss computation and backpropagation for debugging\n",
    "        # _, _, h, w = masks.shape\n",
    "        # loss_mask = torch.zeros_like(outputs)\n",
    "        # loss_mask[:, :, :h, :w] = 1\n",
    "        # loss = F.binary_cross_entropy_with_logits(outputs, masks, reduction='none')\n",
    "        # masked_loss = (loss * loss_mask).sum() / loss_mask.sum()\n",
    "        # optimizer.zero_grad()\n",
    "        # masked_loss.backward()\n",
    "        # optimizer.step()\n",
    "        # train_loss += masked_loss.item()\n",
    "\n",
    "    # Validation\n",
    "    print(f\"Starting validation epoch {epoch+1}\")  # Debug: Ensure entering the validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            print(f\"Validation batch shapes - Images: {images.shape}, Masks: {masks.shape}\")  # Debug: Check input shapes\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            print(f\"Validation batch shapes (on device) - Images: {images.shape}, Masks: {masks.shape}\")  # Debug: Check shapes after moving to device\n",
    "\n",
    "            outputs = model(images)\n",
    "            print(\"Model output shape:\", outputs.shape)  # Debug: Check model output shape\n",
    "            print(\"Target masks shape:\", masks.shape)  # Debug: Check target masks shape\n",
    "\n",
    "            # Temporarily remove loss computation for debugging\n",
    "            # _, _, h, w = masks.shape\n",
    "            # loss_mask = torch.zeros_like(outputs)\n",
    "            # loss_mask[:, :, :h, :w] = 1\n",
    "            # loss = F.binary_cross_entropy_with_logits(outputs, masks, reduction='none')\n",
    "            # masked_loss = (loss * loss_mask).sum() / loss_mask.sum()\n",
    "            # val_loss += masked_loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built to only train on originals while padding degrades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), \"unet_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nightly-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
