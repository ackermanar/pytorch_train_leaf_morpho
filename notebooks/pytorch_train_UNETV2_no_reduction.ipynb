{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define segmentation dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.image_files = sorted([f for f in os.listdir(image_dir) if not f.startswith('.')])\n",
    "        self.mask_files = sorted([f for f in os.listdir(mask_dir) if not f.startswith('.')])\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.mask_files[idx])\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        image = self.transform(image)\n",
    "        mask = self.transform(mask)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the UNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        def double_conv(in_ch, out_ch):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        self.dconv_down1 = double_conv(in_channels, 64)\n",
    "        self.dconv_down2 = double_conv(64, 128)\n",
    "        self.dconv_down3 = double_conv(128, 256)\n",
    "        self.dconv_down4 = double_conv(256, 512)\n",
    "\n",
    "        # Modified maxpool with ceil_mode\n",
    "        self.maxpool = nn.MaxPool2d(2, ceil_mode=True)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.dconv_up3 = double_conv(256 + 512, 256)\n",
    "        self.dconv_up2 = double_conv(128 + 256, 128)\n",
    "        self.dconv_up1 = double_conv(64 + 128, 64)\n",
    "\n",
    "        self.conv_last = nn.Conv2d(64, out_channels, 1)\n",
    "\n",
    "    def crop_tensor(self, target_tensor, tensor_to_crop):\n",
    "        _, _, H, W = tensor_to_crop.size()\n",
    "        return target_tensor[:, :, :H, :W]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder path\n",
    "        conv1 = self.dconv_down1(x)\n",
    "        x = self.maxpool(conv1)\n",
    "\n",
    "        conv2 = self.dconv_down2(x)\n",
    "        x = self.maxpool(conv2)\n",
    "\n",
    "        conv3 = self.dconv_down3(x)\n",
    "        x = self.maxpool(conv3)\n",
    "\n",
    "        # Bottleneck\n",
    "        x = self.dconv_down4(x)\n",
    "\n",
    "        # Decoder path with cropping\n",
    "        x = self.upsample(x)\n",
    "        conv3 = self.crop_tensor(conv3, x)\n",
    "        x = torch.cat([x, conv3], dim=1)\n",
    "        x = self.dconv_up3(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        conv2 = self.crop_tensor(conv2, x)\n",
    "        x = torch.cat([x, conv2], dim=1)\n",
    "        x = self.dconv_up2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        conv1 = self.crop_tensor(conv1, x)\n",
    "        x = torch.cat([x, conv1], dim=1)\n",
    "        x = self.dconv_up1(x)\n",
    "\n",
    "        out = self.conv_last(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a collate function with padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images, masks = zip(*batch)\n",
    "\n",
    "    max_height = max(img.shape[-2] for img in images)\n",
    "    max_width = max(img.shape[-1] for img in images)\n",
    "\n",
    "    def pad_to_size(tensor, target_height, target_width, mode='reflect'):\n",
    "        if tensor.ndim == 3:\n",
    "            tensor = tensor.unsqueeze(0)  # Add batch dimension\n",
    "        _, _, h, w = tensor.shape\n",
    "        pad_h = target_height - h\n",
    "        pad_w = target_width - w\n",
    "        padded_tensor = F.pad(tensor, (0, pad_w, 0, pad_h), mode=mode)\n",
    "        return padded_tensor.squeeze(0)  # Remove batch dimension\n",
    "\n",
    "    padded_images = [pad_to_size(img, max_height, max_width, mode='reflect') for img in images]\n",
    "    padded_masks = [pad_to_size(msk.unsqueeze(0), max_height, max_width, mode='constant') for msk in masks]\n",
    "\n",
    "    # Check if all padded images have the same shape\n",
    "    assert all(img.shape == padded_images[0].shape for img in padded_images), \"Padded images have inconsistent shapes\"\n",
    "\n",
    "    # Check if all padded masks have the same shape \n",
    "    assert all(msk.shape == padded_masks[0].shape for msk in padded_masks), \"Padded masks have inconsistent shapes\"\n",
    "\n",
    "    # Print shapes for debugging\n",
    "    print(\"Padded images shapes:\", [img.shape for img in padded_images])\n",
    "    print(\"Padded masks shapes:\", [msk.shape for msk in padded_masks])\n",
    "\n",
    "    return torch.stack(padded_images), torch.stack(padded_masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images, masks = zip(*batch)\n",
    "\n",
    "    max_height = max(img.shape[-2] for img in images)\n",
    "    max_width = max(img.shape[-1] for img in images)\n",
    "\n",
    "    # Ensure the dimensions are divisible by 8\n",
    "    target_height = ((max_height + 7) // 8) * 8\n",
    "    target_width = ((max_width + 7) // 8) * 8\n",
    "\n",
    "    def pad_to_size(tensor, target_height, target_width, mode='reflect'):\n",
    "        if tensor.ndim == 3:\n",
    "            tensor = tensor.unsqueeze(0)  # Add batch dimension\n",
    "        _, _, h, w = tensor.shape\n",
    "        pad_h = target_height - h\n",
    "        pad_w = target_width - w\n",
    "        padded_tensor = F.pad(tensor, (0, pad_w, 0, pad_h), mode=mode)\n",
    "        return padded_tensor.squeeze(0)  # Remove batch dimension\n",
    "\n",
    "    padded_images = [pad_to_size(img, target_height, target_width, mode='reflect') for img in images]\n",
    "    padded_masks = [pad_to_size(msk.unsqueeze(0), target_height, target_width, mode='constant') for msk in masks]\n",
    "\n",
    "    # Check if all padded images have the same shape\n",
    "    assert all(img.shape == padded_images[0].shape for img in padded_images), \"Padded images have inconsistent shapes\"\n",
    "\n",
    "    # Check if all padded masks have the same shape \n",
    "    assert all(msk.shape == padded_masks[0].shape for msk in padded_masks), \"Padded masks have inconsistent shapes\"\n",
    "\n",
    "    # Print shapes for debugging\n",
    "    print(\"Padded images shapes:\", [img.shape for img in padded_images])\n",
    "    print(\"Padded masks shapes:\", [msk.shape for msk in padded_masks])\n",
    "\n",
    "    return torch.stack(padded_images), torch.stack(padded_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0 reduction update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images, masks = zip(*batch)\n",
    "\n",
    "    max_height = max(img.shape[-2] for img in images)\n",
    "    max_width = max(img.shape[-1] for img in images)\n",
    "\n",
    "    # Ensure the dimensions are divisible by 8\n",
    "    target_height = ((max_height + 7) // 8) * 8\n",
    "    target_width = ((max_width + 7) // 8) * 8\n",
    "\n",
    "    def pad_to_multiple(tensor, target_height, target_width):\n",
    "        _, _, h, w = tensor.shape\n",
    "        pad_h = max(target_height - h, 0)\n",
    "        pad_w = max(target_width - w, 0)\n",
    "        return F.pad(tensor, (0, pad_w, 0, pad_h), mode='reflect')\n",
    "\n",
    "    padded_images = [pad_to_multiple(img, target_height, target_width) for img in images]\n",
    "    padded_masks = [pad_to_multiple(msk.unsqueeze(0), target_height, target_width) for msk in masks]\n",
    "\n",
    "    # Check if all padded images have the same shape\n",
    "    assert all(img.shape == padded_images[0].shape for img in padded_images), \"Padded images have inconsistent shapes\"\n",
    "\n",
    "    # Check if all padded masks have the same shape \n",
    "    assert all(msk.shape == padded_masks[0].shape for msk in padded_masks), \"Padded masks have inconsistent shapes\"\n",
    "\n",
    "    # Print shapes for debugging\n",
    "    print(\"Padded images shapes:\", [img.shape for img in padded_images])\n",
    "    print(\"Padded masks shapes:\", [msk.shape for msk in padded_masks])\n",
    "\n",
    "    return torch.stack(padded_images), torch.stack(padded_masks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## upload train/val sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize datasets\n",
    "train_dataset = SegmentationDataset(\n",
    "    image_dir=\"/Users/aja294/Documents/Hemp_local/leaf_morphometrics/semantic_seg_template/data/train/images\",\n",
    "    mask_dir=\"/Users/aja294/Documents/Hemp_local/leaf_morphometrics/semantic_seg_template/data/train/masks\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = SegmentationDataset(\n",
    "    image_dir=\"/Users/aja294/Documents/Hemp_local/leaf_morphometrics/semantic_seg_template/data/val/images\",\n",
    "    mask_dir=\"/Users/aja294/Documents/Hemp_local/leaf_morphometrics/semantic_seg_template/data/val/masks\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Metal/Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "model = UNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "model = UNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "model = UNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Updated model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 1\n",
      "Padded images shapes: [torch.Size([3, 2528, 2728]), torch.Size([3, 2528, 2728]), torch.Size([3, 2528, 2728]), torch.Size([3, 2528, 2728])]\n",
      "Padded masks shapes: [torch.Size([1, 2528, 2728]), torch.Size([1, 2528, 2728]), torch.Size([1, 2528, 2728]), torch.Size([1, 2528, 2728])]\n",
      "Training batch shapes - Images: torch.Size([4, 3, 2528, 2728]), Masks: torch.Size([4, 1, 2528, 2728])\n",
      "Training batch shapes (on device) - Images: torch.Size([4, 3, 2528, 2728]), Masks: torch.Size([4, 1, 2528, 2728])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Starting training epoch {epoch+1}\")  # Debug: Ensure entering the training loop\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for images, masks in train_loader:\n",
    "        print(f\"Training batch shapes - Images: {images.shape}, Masks: {masks.shape}\")  # Debug: Check input shapes\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        print(f\"Training batch shapes (on device) - Images: {images.shape}, Masks: {masks.shape}\")  # Debug: Check shapes after moving to device\n",
    "\n",
    "        outputs = model(images)\n",
    "        print(\"Model output shape:\", outputs.shape)  # Debug: Check model output shape\n",
    "        print(\"Target masks shape:\", masks.shape)  # Debug: Check target masks shape\n",
    "\n",
    "        # Temporarily remove loss computation and backpropagation for debugging\n",
    "        # _, _, h, w = masks.shape\n",
    "        # loss_mask = torch.zeros_like(outputs)\n",
    "        # loss_mask[:, :, :h, :w] = 1\n",
    "        # loss = F.binary_cross_entropy_with_logits(outputs, masks, reduction='none')\n",
    "        # masked_loss = (loss * loss_mask).sum() / loss_mask.sum()\n",
    "        # optimizer.zero_grad()\n",
    "        # masked_loss.backward()\n",
    "        # optimizer.step()\n",
    "        # train_loss += masked_loss.item()\n",
    "\n",
    "    # Validation\n",
    "    print(f\"Starting validation epoch {epoch+1}\")  # Debug: Ensure entering the validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            print(f\"Validation batch shapes - Images: {images.shape}, Masks: {masks.shape}\")  # Debug: Check input shapes\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            print(f\"Validation batch shapes (on device) - Images: {images.shape}, Masks: {masks.shape}\")  # Debug: Check shapes after moving to device\n",
    "\n",
    "            outputs = model(images)\n",
    "            print(\"Model output shape:\", outputs.shape)  # Debug: Check model output shape\n",
    "            print(\"Target masks shape:\", masks.shape)  # Debug: Check target masks shape\n",
    "\n",
    "            # Temporarily remove loss computation for debugging\n",
    "            # _, _, h, w = masks.shape\n",
    "            # loss_mask = torch.zeros_like(outputs)\n",
    "            # loss_mask[:, :, :h, :w] = 1\n",
    "            # loss = F.binary_cross_entropy_with_logits(outputs, masks, reduction='none')\n",
    "            # masked_loss = (loss * loss_mask).sum() / loss_mask.sum()\n",
    "            # val_loss += masked_loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built to only train on originals while padding degrades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      5\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTraining batch shapes - Images: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m, Masks: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Debug: Check input shapes\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/nightly-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/miniforge3/envs/nightly-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniforge3/envs/nightly-env/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 17\u001b[0m, in \u001b[0;36mcollate_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     14\u001b[0m     pad_w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(target_width \u001b[38;5;241m-\u001b[39m w, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mpad(tensor, (\u001b[38;5;241m0\u001b[39m, pad_w, \u001b[38;5;241m0\u001b[39m, pad_h), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreflect\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m padded_images \u001b[38;5;241m=\u001b[39m [\u001b[43mpad_to_multiple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_width\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m     18\u001b[0m padded_masks \u001b[38;5;241m=\u001b[39m [pad_to_multiple(msk\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), target_height, target_width) \u001b[38;5;28;01mfor\u001b[39;00m msk \u001b[38;5;129;01min\u001b[39;00m masks]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Check if all padded images have the same shape\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[22], line 12\u001b[0m, in \u001b[0;36mcollate_fn.<locals>.pad_to_multiple\u001b[0;34m(tensor, target_height, target_width)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpad_to_multiple\u001b[39m(tensor, target_height, target_width):\n\u001b[0;32m---> 12\u001b[0m     _, _, h, w \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     13\u001b[0m     pad_h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(target_height \u001b[38;5;241m-\u001b[39m h, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     14\u001b[0m     pad_w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(target_width \u001b[38;5;241m-\u001b[39m w, \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 3)"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Starting training epoch {epoch+1}\")  # Debug: Ensure entering the training loop\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for images, masks in train_loader:\n",
    "        print(f\"Training batch shapes - Images: {images.shape}, Masks: {masks.shape}\")  # Debug: Check input shapes\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        print(f\"Training batch shapes (on device) - Images: {images.shape}, Masks: {masks.shape}\")  # Debug: Check shapes after moving to device\n",
    "\n",
    "        outputs = model(images)\n",
    "        print(\"Model output shape:\", outputs.shape)  # Debug: Check model output shape\n",
    "        print(\"Target masks shape:\", masks.shape)  # Debug: Check target masks shape\n",
    "\n",
    "        # Create mask for loss computation\n",
    "        _, _, h, w = masks.shape\n",
    "        loss_mask = torch.zeros_like(outputs)\n",
    "        loss_mask[:, :, :h, :w] = 1\n",
    "\n",
    "        loss = criterion(outputs, masks)\n",
    "        masked_loss = (loss * loss_mask).sum() / loss_mask.sum()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        masked_loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += masked_loss.item()\n",
    "\n",
    "    # Validation\n",
    "    print(f\"Starting validation epoch {epoch+1}\")  # Debug: Ensure entering the validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            print(f\"Validation batch shapes - Images: {images.shape}, Masks: {masks.shape}\")  # Debug: Check input shapes\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            print(f\"Validation batch shapes (on device) - Images: {images.shape}, Masks: {masks.shape}\")  # Debug: Check shapes after moving to device\n",
    "\n",
    "            outputs = model(images)\n",
    "            print(\"Model output shape:\", outputs.shape)  # Debug: Check model output shape\n",
    "            print(\"Target masks shape:\", masks.shape)  # Debug: Check target masks shape\n",
    "\n",
    "            # Create mask for loss computation  \n",
    "            _, _, h, w = masks.shape\n",
    "            loss_mask = torch.zeros_like(outputs)\n",
    "            loss_mask[:, :, :h, :w] = 1\n",
    "\n",
    "            loss = criterion(outputs, masks)\n",
    "            masked_loss = (loss * loss_mask).sum() / loss_mask.sum()\n",
    "            val_loss += masked_loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), \"unet_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nightly-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
